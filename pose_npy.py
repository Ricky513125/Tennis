import argparse
import pickle
import numpy as np
import json
from pathlib import Path
from tqdm import tqdm
import traceback
from multiprocessing import Pool, cpu_count
import logging

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    filename='conversion_errors.log',
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


def validate_pkl_structure(data, filename):
    """æ”¯æŒå¤šäººç‰©å’Œå¤šç§åæ ‡æ ¼å¼çš„éªŒè¯"""
    required_keys = ['keypoint', 'total_frames', 'frame_dir']
    for key in required_keys:
        if key not in data:
            raise ValueError(f"Invalid PKL structure in {filename}: Missing key '{key}'")

    keypoints = data['keypoint']

    # å…è®¸çš„ç»´åº¦æ ¼å¼:
    # - å•äºº: [N, K, 2] æˆ– [N, K, 3]
    # - å¤šäºº: [M, N, K, 2] æˆ– [M, N, K, 3]
    if keypoints.ndim not in (3, 4):
        raise ValueError(
            f"Keypoint format error in {filename}: "
            f"Expected 3 or 4 dimensions, got {keypoints.ndim}"
        )

    # éªŒè¯å¸§æ•°ä¸€è‡´æ€§
    expected_frames = data['total_frames']
    actual_frames = keypoints.shape[-3] if keypoints.ndim == 3 else keypoints.shape[1]
    if actual_frames != expected_frames:
        raise ValueError(
            f"Frame count mismatch in {filename}: "
            f"Keypoints({actual_frames}) vs total_frames({expected_frames})"
        )


def process_single_file(pkl_path, output_root, fill_confidence=1.0, max_people=2):
    """å¤„ç†åŒ…å«å¤šäººç‰©æ•°æ®çš„PKLæ–‡ä»¶"""
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f, encoding='latin1')

        validate_pkl_structure(data, pkl_path.name)

        # è·å–å…³é”®ç‚¹æ•°æ®ç»´åº¦
        keypoints = data['keypoint']
        is_multi_person = keypoints.ndim == 4
        num_people = keypoints.shape[0] if is_multi_person else 1
        num_frames = data['total_frames']
        num_kpts = keypoints.shape[-2]
        coord_dim = keypoints.shape[-1]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        video_id = data['frame_dir']
        output_dir = Path(output_root) / video_id / 'npy'
        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†å½¢çŠ¶æ•°æ®ï¼ˆå…¼å®¹tupleå’Œnumpy arrayï¼‰
        def shape_to_list(shape_data):
            if isinstance(shape_data, (tuple, list)):
                return list(shape_data)
            elif isinstance(shape_data, np.ndarray):
                return shape_data.tolist()
            else:
                return []

        # ç”Ÿæˆå¢å¼ºç‰ˆå…ƒæ•°æ®
        metadata = {
            "source_file": str(pkl_path),
            "structure": {
                "multi_person": is_multi_person,
                "num_people": num_people,
                "num_frames": num_frames,
                "num_keypoints": num_kpts,
                "coordinate_dim": coord_dim,
                "coordinate_meaning": ["x", "y", "confidence"][:coord_dim],
                "confidence_filled": fill_confidence if coord_dim == 2 else None
            },
            "processing_info": {
                "original_resolution": shape_to_list(data.get('original_shape', ())),
                "resized_resolution": shape_to_list(data.get('img_shape', ())),
                "normalized": False,
                "max_people_processed": min(num_people, max_people)
            }
        }

        # ä¿å­˜å…ƒæ•°æ®
        with (output_dir.parent / 'metadata.json').open('w') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # å¤„ç†å¤šäººç‰©æ•°æ®ï¼ˆæœ€å¤šå¤„ç†max_peopleä¸ªï¼‰
        for person_idx in range(min(num_people, max_people)):
            person_dir = output_dir / f"person_{person_idx + 1}"
            person_dir.mkdir(exist_ok=True)

            # æå–å•äººæ•°æ®
            if is_multi_person:
                person_kpts = keypoints[person_idx]  # [N, K, 2/3]
            else:
                person_kpts = keypoints

            # è¡¥å……ç¼ºå¤±çš„ç½®ä¿¡åº¦
            if coord_dim == 2:
                padded_kpts = np.concatenate([
                    person_kpts,
                    np.full((*person_kpts.shape[:-1], 1), fill_confidence)
                ], axis=-1)
            else:
                padded_kpts = person_kpts

            # ä¿å­˜æ¯å¸§æ•°æ®
            for frame_idx, frame_data in enumerate(padded_kpts, start=1):
                np.save(
                    person_dir / f"{frame_idx:06d}.npy",
                    np.round(frame_data.astype(np.float32), 3)
                )

        return True

    except Exception as e:
        error_msg = f"Error processing {pkl_path.name}: {str(e)}\n{traceback.format_exc()}"
        logging.error(error_msg)
        print(f"\n[ERROR] {error_msg.splitlines()[0]}")
        return False


def batch_processor(args):
    """å¤šè¿›ç¨‹å‚æ•°è§£åŒ…"""
    return process_single_file(*args)


def main():
    parser = argparse.ArgumentParser(description='TENNIS å¤šäººç‰©éª¨éª¼æ•°æ®è½¬æ¢å·¥å…·')
    parser.add_argument('--input',
                        type=str,
                        default='/mnt/ssd2/lingyu/Tennis/data/TENNIS/skeletons/f3set-tennis',
                        help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆåŒ…å«.pklæ–‡ä»¶ï¼‰')
    parser.add_argument('--output',
                        type=str,
                        default='/mnt/ssd2/lingyu/Tennis/data/TENNIS/pose',
                        help='è¾“å‡ºæ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--workers',
                        type=int,
                        default=max(1, cpu_count() // 2),
                        help='å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šCPUæ ¸å¿ƒæ•°50%ï¼‰')
    parser.add_argument('--skip_errors',
                        action='store_true',
                        help='é‡åˆ°é”™è¯¯ç»§ç»­å¤„ç†åç»­æ–‡ä»¶')
    parser.add_argument('--fill_confidence',
                        type=float,
                        default=1.0,
                        help='ä¸º2Dåæ ‡è¡¥å……çš„ç½®ä¿¡åº¦å€¼ï¼ˆé»˜è®¤ï¼š1.0ï¼‰')
    parser.add_argument('--max_people',
                        type=int,
                        default=2,
                        help='æœ€å¤§å¤„ç†äººç‰©æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰')

    args = parser.parse_args()

    # å‡†å¤‡æ–‡ä»¶åˆ—è¡¨
    input_dir = Path(args.input)
    pkl_files = list(input_dir.glob('*.pkl'))
    print(f"â–¶ å‘ç° {len(pkl_files)} ä¸ªPKLæ–‡ä»¶å¾…å¤„ç†")

    # åˆ›å»ºè¿›ç¨‹æ± 
    task_args = [(f, args.output, args.fill_confidence, args.max_people) for f in pkl_files]
    success_count = 0

    with Pool(processes=args.workers) as pool:
        results = []
        with tqdm(total=len(pkl_files),
                  desc='è½¬æ¢è¿›åº¦',
                  unit='file',
                  bar_format='{l_bar}{bar:30}{r_bar}',
                  dynamic_ncols=True) as pbar:

            for result in pool.imap_unordered(batch_processor, task_args):
                results.append(result)
                if result:
                    success_count += 1
                else:
                    if not args.skip_errors:
                        print("\nâŒ æ£€æµ‹åˆ°é”™è¯¯ä¸”æœªå¯ç”¨--skip_errorsï¼Œç»ˆæ­¢å¤„ç†...")
                        pool.terminate()
                        break
                pbar.update()
                pbar.set_postfix({
                    'æˆåŠŸç‡': f"{success_count / len(results) * 100:.1f}%",
                    'å·²å¤„ç†': len(results)
                })

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 50)
    print(f"âœ… è½¬æ¢å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(pkl_files) - success_count}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•ç»“æ„ç¤ºä¾‹:")
    print(f"  {args.output}/")
    print(f"  â””â”€â”€ [video_id]/")
    print(f"      â”œâ”€â”€ metadata.json")
    print(f"      â””â”€â”€ npy/")
    print(f"          â”œâ”€â”€ person_1/")
    print(f"          â”‚   â”œâ”€â”€ 000001.npy (shape: [17, 3])")
    print(f"          â”‚   â””â”€â”€ ...")
    print(f"          â””â”€â”€ person_2/")
    print(f"              â””â”€â”€ ...")
    print("=" * 50)


if __name__ == "__main__":
    main()