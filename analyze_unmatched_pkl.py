"""
åˆ†æä¸åœ¨ unlabel.json ä¸­çš„ PKL æ–‡ä»¶ï¼ŒæŸ¥æ‰¾åŒ¹é…å¤±è´¥çš„åŸå› 
"""
import json
import argparse
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
import re


def normalize_video_id(video_id):
    """æ ‡å‡†åŒ– video_idï¼Œç”¨äºåŒ¹é…"""
    # è½¬æ¢ä¸ºå°å†™
    normalized = video_id.lower()
    # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦å·®å¼‚
    normalized = normalized.replace('_', '-')
    normalized = normalized.replace(' ', '-')
    return normalized


def extract_video_id_patterns(pkl_name):
    """ä» PKL æ–‡ä»¶åä¸­æå–å¯èƒ½çš„ video_id æ¨¡å¼"""
    patterns = []
    
    # åŸå§‹åç§°
    patterns.append(pkl_name)
    
    # å°å†™ç‰ˆæœ¬
    patterns.append(pkl_name.lower())
    
    # å¤§å†™ç‰ˆæœ¬
    patterns.append(pkl_name.upper())
    
    # æ›¿æ¢ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
    patterns.append(pkl_name.replace('_', '-'))
    patterns.append(pkl_name.replace('-', '_'))
    
    # ç§»é™¤æ–‡ä»¶æ‰©å±•ååçš„å„ç§å˜ä½“
    base_name = pkl_name
    patterns.append(base_name.replace('_', ''))
    patterns.append(base_name.replace('-', ''))
    
    return patterns


def analyze_unmatched_pkl(unlabel_json_path, skeleton_dir, output_report=None):
    """
    åˆ†æä¸åœ¨ unlabel.json ä¸­çš„ PKL æ–‡ä»¶
    
    Args:
        unlabel_json_path: unlabel.json æ–‡ä»¶è·¯å¾„
        skeleton_dir: skeleton PKL æ–‡ä»¶ç›®å½•
        output_report: å¯é€‰ï¼Œè¾“å‡ºè¯¦ç»†æŠ¥å‘Šçš„æ–‡ä»¶è·¯å¾„
    """
    # è¯»å– unlabel.json
    print(f"ğŸ“– è¯»å– unlabel.json: {unlabel_json_path}")
    with open(unlabel_json_path, 'r') as f:
        unlabel_data = json.load(f)
    
    print(f"ğŸ“Š unlabel.json ä¸­çš„è§†é¢‘æ•°: {len(unlabel_data)}")
    
    # å»ºç«‹ unlabel.json ä¸­çš„ video_id é›†åˆï¼ˆåŒ…å«å„ç§å˜ä½“ï¼‰
    unlabel_video_ids = set()
    unlabel_video_ids_normalized = set()
    unlabel_video_ids_dict = {}  # åŸå§‹ -> æ ‡å‡†åŒ–æ˜ å°„
    
    for item in unlabel_data:
        video_id = item.get("video")
        if video_id:
            unlabel_video_ids.add(video_id)
            normalized = normalize_video_id(video_id)
            unlabel_video_ids_normalized.add(normalized)
            unlabel_video_ids_dict[video_id] = normalized
    
    print(f"ğŸ“‹ å”¯ä¸€ video_id æ•°: {len(unlabel_video_ids)}")
    
    # è·å– skeleton ç›®å½•ä¸­çš„æ‰€æœ‰ PKL æ–‡ä»¶
    skeleton_path = Path(skeleton_dir)
    if not skeleton_path.exists():
        print(f"âŒ Skeleton ç›®å½•ä¸å­˜åœ¨: {skeleton_dir}")
        return
    
    print(f"\nğŸ” æ‰«æ skeleton ç›®å½•: {skeleton_dir}")
    pkl_files = list(skeleton_path.glob("*.pkl"))
    print(f"ğŸ“¦ æ‰¾åˆ° {len(pkl_files)} ä¸ª PKL æ–‡ä»¶")
    
    # åˆ†ææ¯ä¸ª PKL æ–‡ä»¶
    matched_pkl = []
    unmatched_pkl = []
    case_mismatch = []  # å¤§å°å†™ä¸åŒ¹é…
    separator_mismatch = []  # åˆ†éš”ç¬¦ä¸åŒ¹é…ï¼ˆ_ vs -ï¼‰
    partial_match = []  # éƒ¨åˆ†åŒ¹é…
    no_match = []  # å®Œå…¨æ— åŒ¹é…
    
    print("\nğŸ” åˆ†æ PKL æ–‡ä»¶åŒ¹é…æƒ…å†µ...")
    for pkl_file in tqdm(pkl_files, desc="åˆ†æ PKL æ–‡ä»¶"):
        pkl_name = pkl_file.stem  # å»æ‰ .pkl åç¼€
        
        # ç›´æ¥åŒ¹é…
        if pkl_name in unlabel_video_ids:
            matched_pkl.append((pkl_name, "exact"))
            continue
        
        # æ ‡å‡†åŒ–ååŒ¹é…
        pkl_normalized = normalize_video_id(pkl_name)
        if pkl_normalized in unlabel_video_ids_normalized:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹ video_id
            matched_original = None
            for orig_id, norm_id in unlabel_video_ids_dict.items():
                if norm_id == pkl_normalized:
                    matched_original = orig_id
                    break
            
            if matched_original:
                if pkl_name.lower() != matched_original.lower():
                    case_mismatch.append((pkl_name, matched_original, "case"))
                elif pkl_name.replace('_', '-') != matched_original.replace('_', '-'):
                    separator_mismatch.append((pkl_name, matched_original, "separator"))
                else:
                    matched_pkl.append((pkl_name, "normalized"))
            continue
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆæ£€æŸ¥æ˜¯å¦åŒ…å«æˆ–åŒ…å«äºï¼‰
        partial_found = False
        for video_id in unlabel_video_ids:
            # PKL åç§°åŒ…å« video_id æˆ– video_id åŒ…å« PKL åç§°
            if pkl_name in video_id or video_id in pkl_name:
                partial_match.append((pkl_name, video_id, "partial"))
                partial_found = True
                break
        
        if not partial_found:
            no_match.append(pkl_name)
            unmatched_pkl.append(pkl_name)
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š åŒ¹é…åˆ†æç»“æœ")
    print("="*80)
    print(f"âœ… å®Œå…¨åŒ¹é…: {len(matched_pkl)} ä¸ª")
    print(f"âš ï¸  å¤§å°å†™ä¸åŒ¹é…: {len(case_mismatch)} ä¸ª")
    print(f"âš ï¸  åˆ†éš”ç¬¦ä¸åŒ¹é…: {len(separator_mismatch)} ä¸ª")
    print(f"ğŸ” éƒ¨åˆ†åŒ¹é…: {len(partial_match)} ä¸ª")
    print(f"âŒ å®Œå…¨æ— åŒ¹é…: {len(no_match)} ä¸ª")
    print(f"ğŸ“¦ æ€»è®¡æœªåŒ¹é…: {len(unmatched_pkl)} ä¸ª")
    
    # è¾“å‡ºè¯¦ç»†æŠ¥å‘Š
    if output_report:
        with open(output_report, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("PKL æ–‡ä»¶åŒ¹é…åˆ†ææŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"unlabel.json ä¸­çš„è§†é¢‘æ•°: {len(unlabel_data)}\n")
            f.write(f"PKL æ–‡ä»¶æ€»æ•°: {len(pkl_files)}\n")
            f.write(f"å®Œå…¨åŒ¹é…: {len(matched_pkl)}\n")
            f.write(f"å¤§å°å†™ä¸åŒ¹é…: {len(case_mismatch)}\n")
            f.write(f"åˆ†éš”ç¬¦ä¸åŒ¹é…: {len(separator_mismatch)}\n")
            f.write(f"éƒ¨åˆ†åŒ¹é…: {len(partial_match)}\n")
            f.write(f"å®Œå…¨æ— åŒ¹é…: {len(no_match)}\n\n")
            
            if case_mismatch:
                f.write("\n" + "="*80 + "\n")
                f.write("å¤§å°å†™ä¸åŒ¹é…ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰:\n")
                f.write("="*80 + "\n")
                for pkl_name, video_id, _ in case_mismatch[:20]:
                    f.write(f"PKL: {pkl_name}\n")
                    f.write(f"JSON: {video_id}\n")
                    f.write(f"å·®å¼‚: å¤§å°å†™ä¸åŒ\n\n")
            
            if separator_mismatch:
                f.write("\n" + "="*80 + "\n")
                f.write("åˆ†éš”ç¬¦ä¸åŒ¹é…ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰:\n")
                f.write("="*80 + "\n")
                for pkl_name, video_id, _ in separator_mismatch[:20]:
                    f.write(f"PKL: {pkl_name}\n")
                    f.write(f"JSON: {video_id}\n")
                    f.write(f"å·®å¼‚: åˆ†éš”ç¬¦ä¸åŒ (_ vs -)\n\n")
            
            if partial_match:
                f.write("\n" + "="*80 + "\n")
                f.write("éƒ¨åˆ†åŒ¹é…ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰:\n")
                f.write("="*80 + "\n")
                for pkl_name, video_id, _ in partial_match[:20]:
                    f.write(f"PKL: {pkl_name}\n")
                    f.write(f"JSON: {video_id}\n")
                    f.write(f"å…³ç³»: éƒ¨åˆ†åŒ…å«\n\n")
            
            if no_match:
                f.write("\n" + "="*80 + "\n")
                f.write("å®Œå…¨æ— åŒ¹é…çš„ PKL æ–‡ä»¶ï¼ˆå‰50ä¸ªï¼‰:\n")
                f.write("="*80 + "\n")
                for pkl_name in no_match[:50]:
                    f.write(f"{pkl_name}\n")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_report}")
    
    # è¾“å‡ºä¸€äº›ç¤ºä¾‹
    print("\n" + "="*80)
    print("ğŸ“ ç¤ºä¾‹åˆ†æ")
    print("="*80)
    
    if case_mismatch:
        print(f"\nâš ï¸  å¤§å°å†™ä¸åŒ¹é…ç¤ºä¾‹ï¼ˆå…± {len(case_mismatch)} ä¸ªï¼‰:")
        for pkl_name, video_id, _ in case_mismatch[:5]:
            print(f"  PKL: {pkl_name}")
            print(f"  JSON: {video_id}")
            print()
    
    if separator_mismatch:
        print(f"\nâš ï¸  åˆ†éš”ç¬¦ä¸åŒ¹é…ç¤ºä¾‹ï¼ˆå…± {len(separator_mismatch)} ä¸ªï¼‰:")
        for pkl_name, video_id, _ in separator_mismatch[:5]:
            print(f"  PKL: {pkl_name}")
            print(f"  JSON: {video_id}")
            print()
    
    if partial_match:
        print(f"\nğŸ” éƒ¨åˆ†åŒ¹é…ç¤ºä¾‹ï¼ˆå…± {len(partial_match)} ä¸ªï¼‰:")
        for pkl_name, video_id, _ in partial_match[:5]:
            print(f"  PKL: {pkl_name}")
            print(f"  JSON: {video_id}")
            print()
    
    if no_match:
        print(f"\nâŒ å®Œå…¨æ— åŒ¹é…ç¤ºä¾‹ï¼ˆå…± {len(no_match)} ä¸ªï¼‰:")
        for pkl_name in no_match[:10]:
            print(f"  {pkl_name}")
    
    # åˆ†æå‘½åæ¨¡å¼
    print("\n" + "="*80)
    print("ğŸ”¬ å‘½åæ¨¡å¼åˆ†æ")
    print("="*80)
    
    # åˆ†æ unlabel.json ä¸­çš„ video_id å‘½åæ¨¡å¼
    json_patterns = defaultdict(int)
    for video_id in unlabel_video_ids:
        # ç»Ÿè®¡ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦çš„ä½¿ç”¨
        if '_' in video_id and '-' in video_id:
            json_patterns['both'] += 1
        elif '_' in video_id:
            json_patterns['underscore'] += 1
        elif '-' in video_id:
            json_patterns['hyphen'] += 1
        else:
            json_patterns['none'] += 1
    
    print("\nunlabel.json ä¸­çš„å‘½åæ¨¡å¼:")
    for pattern, count in sorted(json_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"  {pattern}: {count} ä¸ª")
    
    # åˆ†ææœªåŒ¹é… PKL çš„å‘½åæ¨¡å¼
    pkl_patterns = defaultdict(int)
    for pkl_name in unmatched_pkl:
        if '_' in pkl_name and '-' in pkl_name:
            pkl_patterns['both'] += 1
        elif '_' in pkl_name:
            pkl_patterns['underscore'] += 1
        elif '-' in pkl_name:
            pkl_patterns['hyphen'] += 1
        else:
            pkl_patterns['none'] += 1
    
    print("\næœªåŒ¹é… PKL æ–‡ä»¶çš„å‘½åæ¨¡å¼:")
    for pattern, count in sorted(pkl_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"  {pattern}: {count} ä¸ª")
    
    return {
        'matched': len(matched_pkl),
        'case_mismatch': len(case_mismatch),
        'separator_mismatch': len(separator_mismatch),
        'partial_match': len(partial_match),
        'no_match': len(no_match),
        'total_pkl': len(pkl_files),
        'total_unlabel': len(unlabel_data)
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åˆ†æä¸åœ¨ unlabel.json ä¸­çš„ PKL æ–‡ä»¶")
    parser.add_argument(
        "--unlabel_json",
        type=str,
        default="/mnt/ssd2/lingyu/Tennis/unlabel.json",
        help="unlabel.json æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--skeleton_dir",
        type=str,
        default="/mnt/ssd2/lingyu/Tennis/data/TENNIS/skeletons/f3set-tennis",
        help="skeleton PKL æ–‡ä»¶ç›®å½•"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="unmatched_pkl_analysis.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    results = analyze_unmatched_pkl(
        args.unlabel_json,
        args.skeleton_dir,
        args.output
    )
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*80)
