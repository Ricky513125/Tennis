"""
è®¡ç®— flow æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”¨äºå½’ä¸€åŒ–
"""
import argparse
import numpy as np
from pathlib import Path
from tqdm import tqdm
import json


def calculate_statistics(npy_files, sample_size=None):
    """
    è®¡ç®— flow æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        npy_files: .npy æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        sample_size: é‡‡æ ·æ•°é‡ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ï¼‰
    
    Returns:
        mean: æ¯ä¸ªé€šé“çš„å‡å€¼ [mean_ch0, mean_ch1]
        std: æ¯ä¸ªé€šé“çš„æ ‡å‡†å·® [std_ch0, std_ch1]
    """
    if sample_size and sample_size < len(npy_files):
        import random
        npy_files = random.sample(npy_files, sample_size)
    
    print(f"ğŸ“Š è®¡ç®— {len(npy_files)} ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯...")
    
    # ç”¨äºç´¯ç§¯ç»Ÿè®¡
    sum_ch0 = 0.0
    sum_ch1 = 0.0
    sum_sq_ch0 = 0.0
    sum_sq_ch1 = 0.0
    total_pixels = 0
    
    # ç”¨äºè®¡ç®—å…¨å±€ min/max
    min_ch0 = float('inf')
    max_ch0 = float('-inf')
    min_ch1 = float('inf')
    max_ch1 = float('-inf')
    
    for npy_file in tqdm(npy_files, desc="å¤„ç†æ–‡ä»¶"):
        try:
            data = np.load(str(npy_file))
            
            # æ£€æŸ¥ç»´åº¦
            if data.ndim != 3:
                print(f"âš ï¸  è·³è¿‡ {npy_file.name}: ç»´åº¦ä¸æ˜¯3ç»´ï¼Œå®é™…ç»´åº¦ {data.shape}")
                continue
            
            # å‡è®¾ç»´åº¦æ˜¯ [C, H, W] = [2, 224, 398] æˆ–ç±»ä¼¼
            if data.shape[0] == 2:
                # [C, H, W] æ ¼å¼
                ch0 = data[0, :, :].flatten()
                ch1 = data[1, :, :].flatten()
            elif data.shape[2] == 2:
                # [H, W, C] æ ¼å¼
                ch0 = data[:, :, 0].flatten()
                ch1 = data[:, :, 1].flatten()
            else:
                print(f"âš ï¸  è·³è¿‡ {npy_file.name}: æ— æ³•è¯†åˆ«çš„ç»´åº¦ {data.shape}")
                continue
            
            # ç´¯ç§¯ç»Ÿè®¡
            n_pixels = len(ch0)
            sum_ch0 += ch0.sum()
            sum_ch1 += ch1.sum()
            sum_sq_ch0 += (ch0 ** 2).sum()
            sum_sq_ch1 += (ch1 ** 2).sum()
            total_pixels += n_pixels
            
            # æ›´æ–° min/max
            min_ch0 = min(min_ch0, ch0.min())
            max_ch0 = max(max_ch0, ch0.max())
            min_ch1 = min(min_ch1, ch1.min())
            max_ch1 = max(max_ch1, ch1.max())
            
        except Exception as e:
            print(f"âŒ å¤„ç† {npy_file.name} æ—¶å‡ºé”™: {e}")
            continue
    
    if total_pixels == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
        return None, None
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean_ch0 = sum_ch0 / total_pixels
    mean_ch1 = sum_ch1 / total_pixels
    
    # æ ‡å‡†å·®å…¬å¼: sqrt(E[X^2] - E[X]^2)
    std_ch0 = np.sqrt(sum_sq_ch0 / total_pixels - mean_ch0 ** 2)
    std_ch1 = np.sqrt(sum_sq_ch1 / total_pixels - mean_ch1 ** 2)
    
    mean = [float(mean_ch0), float(mean_ch1)]
    std = [float(std_ch0), float(std_ch1)]
    
    return mean, std, {
        'min': [float(min_ch0), float(min_ch1)],
        'max': [float(max_ch0), float(max_ch1)],
        'total_files': len(npy_files),
        'total_pixels': total_pixels,
    }


def main():
    parser = argparse.ArgumentParser(description='è®¡ç®— flow æ•°æ®çš„å½’ä¸€åŒ–å‚æ•°')
    parser.add_argument('--input',
                        type=str,
                        default='/mnt/ssd2/lingyu/Tennis/data/TENNIS/tennis_flows',
                        help='flow æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--sample',
                        type=int,
                        default=None,
                        help='é‡‡æ ·æ–‡ä»¶æ•°é‡ï¼ˆNone è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ï¼‰')
    parser.add_argument('--output',
                        type=str,
                        default=None,
                        help='è¾“å‡ºç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    flow_dir = Path(args.input)
    
    if not flow_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {flow_dir}")
        return
    
    # æ”¶é›†æ‰€æœ‰ .npy æ–‡ä»¶
    print(f"ğŸ” æœç´¢ .npy æ–‡ä»¶...")
    npy_files = []
    video_dirs = [d for d in flow_dir.iterdir() if d.is_dir()]
    
    for video_dir in video_dirs:
        npy_files.extend(list(video_dir.glob("pair_*.npy")))
    
    if len(npy_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° .npy æ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(npy_files)} ä¸ª .npy æ–‡ä»¶")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean, std, stats = calculate_statistics(npy_files, sample_size=args.sample)
    
    if mean is None:
        return
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š Flow æ•°æ®ç»Ÿè®¡ç»“æœ")
    print("=" * 80)
    print(f"\nğŸ“ å¤„ç†çš„æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"ğŸ“ æ€»åƒç´ æ•°: {stats['total_pixels']:,}")
    print(f"\nğŸ“ˆ æ•°å€¼èŒƒå›´:")
    print(f"   é€šé“ 0 (xæ–¹å‘): [{stats['min'][0]:.6f}, {stats['max'][0]:.6f}]")
    print(f"   é€šé“ 1 (yæ–¹å‘): [{stats['min'][1]:.6f}, {stats['max'][1]:.6f}]")
    print(f"\nğŸ“Š å½’ä¸€åŒ–å‚æ•°:")
    print(f"   Mean: [{mean[0]:.6f}, {mean[1]:.6f}]")
    print(f"   Std:  [{std[0]:.6f}, {std[1]:.6f}]")
    print("\n" + "=" * 80)
    
    # è¾“å‡ºé…ç½®æ–‡ä»¶æ ¼å¼
    print("\nğŸ’¡ é…ç½®æ–‡ä»¶æ ¼å¼ (configs/data_module/modality/flow.yaml):")
    print("```yaml")
    print(f"mean: [{mean[0]:.6f}, {mean[1]:.6f}]")
    print(f"std: [{std[0]:.6f}, {std[1]:.6f}]")
    print("```")
    
    # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        output_data = {
            'mean': mean,
            'std': std,
            'statistics': stats,
            'config_format': {
                'mean': mean,
                'std': std,
            }
        }
        with open(args.output, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nğŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
