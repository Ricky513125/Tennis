"""
è®¡ç®— skeleton çƒ­å›¾æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œç”¨äºå½’ä¸€åŒ–
"""
import argparse
import numpy as np
import pickle
from pathlib import Path
from tqdm import tqdm
import json
import random


def keypoints_to_heatmap(keypoints, H=56, W=98, sigma=2.0):
    """
    å°†å…³é”®ç‚¹è½¬æ¢ä¸ºçƒ­å›¾
    keypoints: [K, 3] (x_norm, y_norm, confidence)ï¼Œåæ ‡å·²å½’ä¸€åŒ–åˆ° [0, 1]
    è¿”å›: [K, H, W] çƒ­å›¾
    æ³¨æ„ï¼šä½¿ç”¨å®½å±æ ¼å¼ (H=56, W=98) ä»¥ä¿æŒå®½é«˜æ¯”ï¼ŒåŒ¹é…æœ€ç»ˆå°ºå¯¸ [224, 384]
    """
    K = keypoints.shape[0]
    heatmap = np.zeros((K, H, W), dtype=np.float32)
    
    # åæ ‡å·²ç»å½’ä¸€åŒ–åˆ° [0, 1]
    x_coords = keypoints[:, 0]  # [0, 1]
    y_coords = keypoints[:, 1]  # [0, 1]
    confidences = keypoints[:, 2]
    
    # è½¬æ¢ä¸ºçƒ­å›¾åæ ‡
    x_centers = x_coords * W  # è½¬æ¢ä¸ºçƒ­å›¾åæ ‡
    y_centers = y_coords * H
    
    # åˆ›å»ºé«˜æ–¯çƒ­å›¾
    for k in range(K):
        if confidences[k] > 0.1:  # åªå¤„ç†ç½®ä¿¡åº¦å¤§äºé˜ˆå€¼çš„ç‚¹
            x_center = x_centers[k]
            y_center = y_centers[k]
            
            # åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ
            y_grid, x_grid = np.ogrid[:H, :W]
            gaussian = np.exp(-((x_grid - x_center)**2 + (y_grid - y_center)**2) / (2 * sigma**2))
            gaussian = gaussian * confidences[k]  # ä¹˜ä»¥ç½®ä¿¡åº¦
            
            heatmap[k] = np.maximum(heatmap[k], gaussian)
    
    return heatmap


def load_skeleton_from_pkl(pkl_path, frame_name):
    """
    ä» PKL æ–‡ä»¶åŠ è½½æŒ‡å®šå¸§çš„ skeleton æ•°æ®ï¼ˆä¸å®é™…æ•°æ®åŠ è½½é€»è¾‘ä¸€è‡´ï¼‰
    
    Args:
        pkl_path: PKL æ–‡ä»¶è·¯å¾„
        frame_name: å¸§å·ï¼ˆä»1å¼€å§‹ï¼‰
    
    Returns:
        keypoints: [K, 3] numpy array (x_norm, y_norm, confidence)ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    try:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f, encoding='latin1')
        
        keypoints = data['keypoint']  # [M, N, K, 2] æˆ– [N, K, 2]
        keypoint_scores = data.get('keypoint_score', None)  # [M, N, K] æˆ– [N, K]
        total_frames = data['total_frames']
        img_shape = data.get('img_shape', (720, 1280))  # (H, W)
        
        # ç¡®ä¿ frame_name åœ¨æœ‰æ•ˆèŒƒå›´å†…
        frame_idx = min(max(0, int(frame_name) - 1), total_frames - 1)
        
        # å¤„ç†å¤šäººæƒ…å†µï¼šå–ç¬¬ä¸€ä¸ªäººï¼ˆperson_idx=0ï¼‰
        if keypoints.ndim == 4:  # [M, N, K, 2] - å¤šäºº
            frame_kpts = keypoints[0, frame_idx]  # [K, 2]
            if keypoint_scores is not None and keypoint_scores.ndim == 3:
                frame_scores = keypoint_scores[0, frame_idx]  # [K]
            else:
                frame_scores = np.ones(frame_kpts.shape[0], dtype=np.float32)
        elif keypoints.ndim == 3:  # [N, K, 2] - å•äºº
            frame_kpts = keypoints[frame_idx]  # [K, 2]
            if keypoint_scores is not None and keypoint_scores.ndim == 2:
                frame_scores = keypoint_scores[frame_idx]  # [K]
            else:
                frame_scores = np.ones(frame_kpts.shape[0], dtype=np.float32)
        else:
            return None
        
        # å½’ä¸€åŒ–åæ ‡åˆ° [0, 1] èŒƒå›´ï¼ˆåŸºäºåŸå§‹å›¾åƒå°ºå¯¸ï¼‰
        H, W = img_shape
        frame_kpts_normalized = frame_kpts.copy().astype(np.float32)
        frame_kpts_normalized[:, 0] = frame_kpts_normalized[:, 0] / W  # x åæ ‡å½’ä¸€åŒ–
        frame_kpts_normalized[:, 1] = frame_kpts_normalized[:, 1] / H  # y åæ ‡å½’ä¸€åŒ–
        
        # åˆå¹¶åæ ‡å’Œç½®ä¿¡åº¦ï¼š[K, 2] + [K] -> [K, 3]
        frame_scores = frame_scores.astype(np.float32).reshape(-1, 1)
        frame_kpts_with_score = np.concatenate([frame_kpts_normalized, frame_scores], axis=-1)
        
        return frame_kpts_with_score  # [K, 3] (x_norm, y_norm, confidence)
        
    except Exception as e:
        return None


def calculate_statistics(pkl_files, skeleton_dir, unlabel_json_path, sample_size=None, num_frames_per_video=16):
    """
    è®¡ç®— skeleton çƒ­å›¾çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        pkl_files: PKL æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        skeleton_dir: skeleton ç›®å½•
        unlabel_json_path: unlabel JSON æ–‡ä»¶è·¯å¾„
        sample_size: é‡‡æ ·è§†é¢‘æ•°é‡ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ï¼‰
        num_frames_per_video: æ¯ä¸ªè§†é¢‘é‡‡æ ·çš„å¸§æ•°
    
    Returns:
        mean: æ¯ä¸ªé€šé“çš„å‡å€¼ [mean_ch0, mean_ch1, ..., mean_ch16] (17ä¸ªé€šé“)
        std: æ¯ä¸ªé€šé“çš„æ ‡å‡†å·® [std_ch0, std_ch1, ..., std_ch16] (17ä¸ªé€šé“)
    """
    # åŠ è½½ unlabel JSON ä»¥è·å–è§†é¢‘ä¿¡æ¯
    try:
        with open(unlabel_json_path, 'r') as f:
            unlabel_data = json.load(f)
        video_dict = {item['video']: item for item in unlabel_data}
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½ unlabel JSON: {e}")
        video_dict = {}
    
    if sample_size and sample_size < len(pkl_files):
        pkl_files = random.sample(pkl_files, sample_size)
    
    print(f"ğŸ“Š è®¡ç®— {len(pkl_files)} ä¸ª PKL æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯...")
    
    # ç”¨äºç´¯ç§¯ç»Ÿè®¡ï¼ˆ17ä¸ªé€šé“ï¼‰
    K = 17  # å…³é”®ç‚¹æ•°é‡
    sums = np.zeros(K, dtype=np.float64)
    sum_sqs = np.zeros(K, dtype=np.float64)
    total_pixels = 0
    
    # ç”¨äºè®¡ç®—å…¨å±€ min/max
    mins = np.full(K, float('inf'))
    maxs = np.full(K, float('-inf'))
    
    processed_files = 0
    processed_frames = 0
    
    for pkl_file in tqdm(pkl_files, desc="å¤„ç†æ–‡ä»¶"):
        try:
            # å°è¯•ä»æ–‡ä»¶åæå– video_id
            video_id = pkl_file.stem
            
            # å°è¯•ä» unlabel JSON è·å–è§†é¢‘ä¿¡æ¯
            video_info = video_dict.get(video_id, None)
            if video_info is None:
                # å°è¯•åŒ¹é…éƒ¨åˆ†æ–‡ä»¶å
                for vid, info in video_dict.items():
                    if vid in video_id or video_id in vid:
                        video_info = info
                        break
            
            # ç¡®å®šè¦é‡‡æ ·çš„å¸§æ•°
            if video_info:
                num_frames = min(num_frames_per_video, video_info.get('num_frames', num_frames_per_video))
                # å‡åŒ€é‡‡æ ·å¸§
                frame_indices = np.linspace(1, num_frames, num_frames_per_video, dtype=int)
            else:
                # å¦‚æœæ²¡æœ‰è§†é¢‘ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤é‡‡æ ·
                frame_indices = np.linspace(1, 100, num_frames_per_video, dtype=int)
            
            # å¤„ç†æ¯ä¸€å¸§
            for frame_idx in frame_indices:
                keypoints = load_skeleton_from_pkl(pkl_file, frame_idx)
                if keypoints is None:
                    continue
                
                # è½¬æ¢ä¸ºçƒ­å›¾ [K, H, W]
                # ä½¿ç”¨å®½å±æ ¼å¼ä»¥ä¿æŒå®½é«˜æ¯”ï¼š56Ã—98 (å®½é«˜æ¯” â‰ˆ 1:1.75ï¼ŒåŒ¹é… 224Ã—384)
                heatmap = keypoints_to_heatmap(keypoints, H=56, W=98, sigma=2.0)
                
                # Resize åˆ°ç›®æ ‡å°ºå¯¸ [224, 384]ï¼ˆä½¿ç”¨åŒçº¿æ€§æ’å€¼ï¼‰ä»¥ä¸ RGB/Flow ä½ç½®å¯¹åº”
                # ä½¿ç”¨ PIL æˆ– numpy è¿›è¡Œ resize
                if heatmap.shape[1] != 224 or heatmap.shape[2] != 384:
                    # ä½¿ç”¨ numpy å’Œç®€å•çš„æ’å€¼æ–¹æ³•
                    # æˆ–è€…ä½¿ç”¨ PILï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    try:
                        from PIL import Image
                        # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ« resize
                        resized_heatmap = np.zeros((heatmap.shape[0], 224, 384), dtype=np.float32)
                        for k in range(heatmap.shape[0]):
                            img = Image.fromarray(heatmap[k])
                            img_resized = img.resize((384, 224), Image.BILINEAR)  # PIL resize ä½¿ç”¨ (W, H)
                            resized_heatmap[k] = np.array(img_resized)
                        heatmap = resized_heatmap
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰ PILï¼Œä½¿ç”¨ç®€å•çš„æœ€è¿‘é‚»æ’å€¼
                        # è®¡ç®—ç¼©æ”¾å› å­
                        scale_h = 224 / heatmap.shape[1]
                        scale_w = 384 / heatmap.shape[2]
                        # åˆ›å»ºæ–°æ•°ç»„
                        resized_heatmap = np.zeros((heatmap.shape[0], 224, 384), dtype=np.float32)
                        for k in range(heatmap.shape[0]):
                            for i in range(224):
                                for j in range(384):
                                    src_i = int(i / scale_h)
                                    src_j = int(j / scale_w)
                                    resized_heatmap[k, i, j] = heatmap[k, src_i, src_j]
                        heatmap = resized_heatmap
                
                # ç´¯ç§¯ç»Ÿè®¡ï¼ˆå¯¹æ¯ä¸ªé€šé“åˆ†åˆ«è®¡ç®—ï¼‰
                for k in range(K):
                    channel_data = heatmap[k, :, :].flatten()
                    n_pixels = len(channel_data)
                    
                    sums[k] += channel_data.sum()
                    sum_sqs[k] += (channel_data ** 2).sum()
                    total_pixels += n_pixels
                    
                    # æ›´æ–° min/max
                    mins[k] = min(mins[k], channel_data.min())
                    maxs[k] = max(maxs[k], channel_data.max())
                
                processed_frames += 1
            
            processed_files += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç† {pkl_file.name} æ—¶å‡ºé”™: {e}")
            continue
    
    if total_pixels == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®")
        return None, None, None
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæ¯ä¸ªé€šé“ï¼‰
    mean_per_channel = sums / (total_pixels / K)  # æ¯ä¸ªé€šé“çš„æ€»åƒç´ æ•°
    std_per_channel = np.sqrt(sum_sqs / (total_pixels / K) - mean_per_channel ** 2)
    
    mean = [float(x) for x in mean_per_channel]
    std = [float(x) for x in std_per_channel]
    
    stats = {
        'min': [float(x) for x in mins],
        'max': [float(x) for x in maxs],
        'total_files': processed_files,
        'total_frames': processed_frames,
        'total_pixels': total_pixels,
    }
    
    return mean, std, stats


def main():
    parser = argparse.ArgumentParser(description='è®¡ç®— skeleton çƒ­å›¾æ•°æ®çš„å½’ä¸€åŒ–å‚æ•°')
    parser.add_argument('--skeleton-dir',
                        type=str,
                        default='/mnt/ssd2/lingyu/Tennis/data/TENNIS/skeletons/f3set-tennis',
                        help='skeleton PKL æ–‡ä»¶ç›®å½•è·¯å¾„')
    parser.add_argument('--unlabel-json',
                        type=str,
                        default='/mnt/ssd2/lingyu/Tennis/unlabel.json',
                        help='unlabel JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sample',
                        type=int,
                        default=1000,
                        help='é‡‡æ ·è§†é¢‘æ•°é‡ï¼ˆé»˜è®¤ 1000ï¼‰')
    parser.add_argument('--frames-per-video',
                        type=int,
                        default=16,
                        help='æ¯ä¸ªè§†é¢‘é‡‡æ ·çš„å¸§æ•°ï¼ˆé»˜è®¤ 16ï¼‰')
    parser.add_argument('--output',
                        type=str,
                        default=None,
                        help='è¾“å‡ºç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    skeleton_dir = Path(args.skeleton_dir)
    
    if not skeleton_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {skeleton_dir}")
        return
    
    # æ”¶é›†æ‰€æœ‰ .pkl æ–‡ä»¶
    print(f"ğŸ” æœç´¢ .pkl æ–‡ä»¶...")
    pkl_files = list(skeleton_dir.glob("*.pkl"))
    
    if len(pkl_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° .pkl æ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(pkl_files)} ä¸ª .pkl æ–‡ä»¶")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean, std, stats = calculate_statistics(
        pkl_files, 
        skeleton_dir, 
        args.unlabel_json,
        sample_size=args.sample,
        num_frames_per_video=args.frames_per_video
    )
    
    if mean is None:
        return
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š Skeleton çƒ­å›¾æ•°æ®ç»Ÿè®¡ç»“æœ")
    print("=" * 80)
    print(f"\nğŸ“ å¤„ç†çš„æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"ğŸ¬ å¤„ç†çš„å¸§æ•°: {stats['total_frames']}")
    print(f"ğŸ“ æ€»åƒç´ æ•°: {stats['total_pixels']:,}")
    
    print(f"\nğŸ“ˆ æ•°å€¼èŒƒå›´ (æ¯ä¸ªå…³é”®ç‚¹é€šé“):")
    for k in range(17):
        print(f"   å…³é”®ç‚¹ {k:2d}: [{stats['min'][k]:.6f}, {stats['max'][k]:.6f}]")
    
    print(f"\nğŸ“Š å½’ä¸€åŒ–å‚æ•° (æ¯ä¸ªå…³é”®ç‚¹é€šé“):")
    print(f"   Mean: {mean}")
    print(f"   Std:  {std}")
    print("\n" + "=" * 80)
    
    # è¾“å‡ºé…ç½®æ–‡ä»¶æ ¼å¼
    print("\nğŸ’¡ é…ç½®æ–‡ä»¶æ ¼å¼ (configs/data_module/modality/skeleton.yaml):")
    print("```yaml")
    print("mean: [")
    for i in range(0, 17, 7):
        end = min(i + 7, 17)
        values = ", ".join([f"{mean[j]:.6f}" for j in range(i, end)])
        if end < 17:
            print(f"    {values},")
        else:
            print(f"    {values}")
    print("]")
    print("std: [")
    for i in range(0, 17, 7):
        end = min(i + 7, 17)
        values = ", ".join([f"{std[j]:.6f}" for j in range(i, end)])
        if end < 17:
            print(f"    {values},")
        else:
            print(f"    {values}")
    print("]")
    print("```")
    
    # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        output_data = {
            'mean': mean,
            'std': std,
            'statistics': stats,
            'config_format': {
                'mean': mean,
                'std': std,
            }
        }
        with open(args.output, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nğŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
