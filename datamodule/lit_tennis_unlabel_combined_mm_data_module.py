"""
Tennis æ•°æ®é›†çš„å¤šæ¨¡æ€æ•°æ®æ¨¡å—ï¼ˆRGBã€Flowã€Skeletonï¼‰
ç”¨äºå¤šæ¨¡æ€è’¸é¦è®­ç»ƒ
"""
import pytorch_lightning as pl
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data.sampler import BatchSampler

from datamodule.dataset.tennis_unlabel_combined_multimodal_dataset import (
    TennisUnlabelCombinedMMDataset,
)
from datamodule.utils.augmentation import (
    DataAugmentationForUnlabelMM,
    DataAugmentationForUnlabelRGB,
    DataAugmentationForVideoMAERGB,
    MaskGeneration,
)
from datamodule.utils.episodic_batch_sampler import EpisodicBatchSampler
from netscripts.get_fewshot_eval_dataset import get_fewshot_eval_dataset


class TennisUnlabelCombinedMMDataModule(pl.LightningDataModule):
    def __init__(self, cfg):
        super(TennisUnlabelCombinedMMDataModule, self).__init__()
        self.cfg = cfg
        self.data_module_cfg = cfg.data_module
        self.n_way = cfg.data_module.n_way
        self.k_shot = cfg.data_module.k_shot
        self.q_sample = cfg.data_module.q_sample
        self.episodes = cfg.data_module.episodes
        self.eval_batch_size = self.n_way * (self.k_shot + self.q_sample)
        
        # transform
        self.mask_gen = MaskGeneration(cfg.data_module)
        self.transform_train_rgb = DataAugmentationForUnlabelRGB(cfg.data_module)
        self.transform_train_flow = DataAugmentationForUnlabelMM(
            cfg.data_module, mean=cfg.data_module.mean[1], std=cfg.data_module.std[1], mode="flow"
        )
        self.transform_train_skeleton = DataAugmentationForUnlabelMM(
            cfg.data_module, mean=cfg.data_module.mean[2], std=cfg.data_module.std[2], mode="skeleton"
        )
        
        self.transform_train = [
            self.transform_train_rgb,
            self.transform_train_flow,
            self.transform_train_skeleton,
        ]
        
        # è¯„ä¼°ç”¨çš„ transformï¼ˆåªä½¿ç”¨ RGBï¼‰
        # ä½¿ç”¨ DataAugmentationForUnlabelRGBï¼Œç¡®ä¿ä¸è®­ç»ƒæ—¶ä½¿ç”¨ç›¸åŒçš„ 224x384 å°ºå¯¸
        # ä»é…ç½®ä¸­è·å– RGB çš„ input_size: [224, 384]
        rgb_input_size = None
        if hasattr(cfg.data_module, 'input_size'):
            if isinstance(cfg.data_module.input_size, list) and len(cfg.data_module.input_size) > 0:
                # å¤šæ¨¡æ€é…ç½®ï¼šinput_size æ˜¯åˆ—è¡¨ [[224, 384], [224, 384], [224, 384]]
                # å–ç¬¬ä¸€ä¸ªï¼ˆRGBï¼‰
                if isinstance(cfg.data_module.input_size[0], (list, tuple)):
                    rgb_input_size = list(cfg.data_module.input_size[0])
                else:
                    rgb_input_size = cfg.data_module.input_size[0]
        
        # åˆ›å»ºè¯„ä¼°ç”¨çš„ RGB transformï¼Œä½¿ç”¨ weak_augï¼ˆåŒ…å« VideoCenterCrop åˆ° 224x384ï¼‰
        self.transform_eval_rgb = DataAugmentationForUnlabelRGB(
            cfg.data_module,
            input_size=rgb_input_size,  # [224, 384]
            mean=cfg.data_module.mean[0] if isinstance(cfg.data_module.mean, list) else cfg.data_module.mean,
            std=cfg.data_module.std[0] if isinstance(cfg.data_module.std, list) else cfg.data_module.std,
        )
        # è¯„ä¼°æ—¶åªä½¿ç”¨ weak_augï¼ˆä¸è¿›è¡Œéšæœºç¿»è½¬ï¼Œä¿æŒä¸€è‡´æ€§ï¼‰
        # ä½† TennisFewshotEvalDataset ä¼šè°ƒç”¨ transform((frames, None))ï¼Œéœ€è¦é€‚é…
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…ç±»ï¼Œä½¿å…¶å…¼å®¹è¯„ä¼°æ•°æ®é›†çš„è°ƒç”¨æ–¹å¼
        # TennisFewshotEvalDataset æœŸæœ›è¾“å‡ºæ ¼å¼: [T*C, H, W]
        import torch
        from datamodule.utils.augmentation import ToTensor
        from PIL import Image as PILImage
        from torchvision import transforms
        
        class VideoCenterCrop:
            """è¯„ä¼°æ—¶ä½¿ç”¨çš„å±…ä¸­è£å‰ªç±»ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
            def __init__(self, size):
                # size å¯èƒ½æ˜¯ [H, W] åˆ—è¡¨æˆ–å•ä¸ªæ•´æ•°
                if isinstance(size, (list, tuple)) and len(size) == 2:
                    self.size = tuple(size)  # (H, W)
                elif isinstance(size, int):
                    self.size = (size, size)
                else:
                    self.size = (224, 384)  # é»˜è®¤å€¼
                self.target_H, self.target_W = self.size
            
            def __call__(self, tensor):
                # tensor: [T, C, H, W]
                # å¯¹æ¯ä¸€å¸§åˆ†åˆ«åº”ç”¨ CenterCrop
                T, C, H, W = tensor.shape
                cropped_frames = []
                
                for t in range(T):
                    frame = tensor[t]  # [C, H, W]
                    
                    # å¦‚æœå°ºå¯¸å·²ç»åŒ¹é…ï¼Œç›´æ¥è¿”å›
                    if H == self.target_H and W == self.target_W:
                        cropped_frames.append(frame)
                        continue
                    
                    # å±…ä¸­è£å‰ªï¼šè®¡ç®—è£å‰ªèµ·å§‹ä½ç½®
                    if H != self.target_H:
                        start_h = (H - self.target_H) // 2
                        end_h = start_h + self.target_H
                    else:
                        start_h = 0
                        end_h = H
                    
                    if W != self.target_W:
                        start_w = (W - self.target_W) // 2
                        end_w = start_w + self.target_W
                    else:
                        start_w = 0
                        end_w = W
                    
                    # æ‰§è¡Œè£å‰ª: [C, H, W] -> [C, target_H, target_W]
                    cropped_frame = frame[:, start_h:end_h, start_w:end_w]
                    
                    # å¦‚æœè£å‰ªåå°ºå¯¸ä»ä¸åŒ¹é…ï¼ˆå¯èƒ½å› ä¸ºåŸå§‹å°ºå¯¸å°äºç›®æ ‡å°ºå¯¸ï¼‰ï¼Œè¿›è¡Œ resize
                    if cropped_frame.shape[1] != self.target_H or cropped_frame.shape[2] != self.target_W:
                        # è½¬æ¢ä¸º PIL Image è¿›è¡Œ resize
                        frame_pil = transforms.ToPILImage()(cropped_frame)
                        cropped_frame = transforms.ToTensor()(
                            frame_pil.resize((self.target_W, self.target_H), PILImage.BILINEAR)  # PIL ä½¿ç”¨ (W, H)
                        )
                    
                    cropped_frames.append(cropped_frame)
                
                return torch.stack(cropped_frames, dim=0)  # [T, C, H, W]
        
        class EvalTransformWrapper:
            def __init__(self, base_transform, input_size, mean, std):
                self.base_transform = base_transform
                self.input_size = input_size  # [224, 384]
                # ç¡®ä¿ mean å’Œ std æ˜¯ PyTorch tensor
                # å¤„ç† omegaconf.ListConfig æˆ–å…¶ä»–ç±»å‹
                if not isinstance(mean, torch.Tensor):
                    if hasattr(mean, '__iter__') and not isinstance(mean, str):
                        mean = list(mean)  # è½¬æ¢ä¸º Python list
                    self.mean = torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1)
                else:
                    self.mean = mean.view(-1, 1, 1)
                
                if not isinstance(std, torch.Tensor):
                    if hasattr(std, '__iter__') and not isinstance(std, str):
                        std = list(std)  # è½¬æ¢ä¸º Python list
                    self.std = torch.tensor(std, dtype=torch.float32).view(-1, 1, 1)
                else:
                    self.std = std.view(-1, 1, 1)
                
                # è¯„ä¼°æ—¶åªè¿›è¡Œå±…ä¸­è£å‰ªå’Œå½’ä¸€åŒ–ï¼Œä¸è¿›è¡Œéšæœºç¿»è½¬
                self.eval_transform = transforms.Compose([
                    ToTensor(),  # PIL Image åˆ—è¡¨ -> [T, C, H, W]
                    VideoCenterCrop(self.input_size),  # [T, C, H, W] -> [T, C, H, W] (å±…ä¸­è£å‰ªåˆ° 224x384)
                ])
            
            def _normalize_tensor(self, tensor):
                # tensor: [T, C, H, W]
                # mean/std: [C, 1, 1] -> [1, C, 1, 1]
                mean = self.mean.view(1, -1, 1, 1)
                std = self.std.view(1, -1, 1, 1)
                return (tensor - mean) / std
            
            def __call__(self, frames_tuple):
                # frames_tuple æ˜¯ (frames, None) æ ¼å¼
                frames, _ = frames_tuple
                # åº”ç”¨è¯„ä¼° transform: ToTensor + VideoCenterCrop
                frames_tensor = self.eval_transform(frames)  # [T, C, H, W]
                # å½’ä¸€åŒ–
                frames_tensor = self._normalize_tensor(frames_tensor)  # [T, C, H, W]
                # è½¬æ¢ä¸º TennisFewshotEvalDataset æœŸæœ›çš„æ ¼å¼: [T*C, H, W]
                T, C, H, W = frames_tensor.shape
                frames_reshaped = frames_tensor.view(T * C, H, W)  # [T*C, H, W]
                return frames_reshaped, None
        
        # æ­£ç¡®æå– RGB çš„ mean å’Œ std
        # cfg.data_module.mean æ˜¯åˆ—è¡¨: [[RGB_mean], [Flow_mean], [Skeleton_mean]]
        if hasattr(cfg.data_module, 'mean'):
            if isinstance(cfg.data_module.mean, (list, tuple)) and len(cfg.data_module.mean) > 0:
                rgb_mean_raw = cfg.data_module.mean[0]
                # å¤„ç† omegaconf.ListConfig æˆ–å…¶ä»–ç±»å‹
                if hasattr(rgb_mean_raw, '__iter__') and not isinstance(rgb_mean_raw, str):
                    rgb_mean = list(rgb_mean_raw)
                else:
                    rgb_mean = rgb_mean_raw
            else:
                rgb_mean = cfg.data_module.mean
        else:
            rgb_mean = [0.485, 0.456, 0.406]  # ImageNet é»˜è®¤å€¼
        
        if hasattr(cfg.data_module, 'std'):
            if isinstance(cfg.data_module.std, (list, tuple)) and len(cfg.data_module.std) > 0:
                rgb_std_raw = cfg.data_module.std[0]
                # å¤„ç† omegaconf.ListConfig æˆ–å…¶ä»–ç±»å‹
                if hasattr(rgb_std_raw, '__iter__') and not isinstance(rgb_std_raw, str):
                    rgb_std = list(rgb_std_raw)
                else:
                    rgb_std = rgb_std_raw
            else:
                rgb_std = cfg.data_module.std
        else:
            rgb_std = [0.229, 0.224, 0.225]  # ImageNet é»˜è®¤å€¼
        
        # ç¡®ä¿ rgb_mean å’Œ rgb_std æ˜¯é•¿åº¦ä¸º 3 çš„åˆ—è¡¨ï¼ˆRGB æœ‰ 3 ä¸ªé€šé“ï¼‰
        if not isinstance(rgb_mean, (list, tuple)) or len(rgb_mean) != 3:
            logger.warning(f"RGB mean length is {len(rgb_mean) if isinstance(rgb_mean, (list, tuple)) else 'not a list'}, expected 3. Using ImageNet defaults.")
            rgb_mean = [0.485, 0.456, 0.406]
        
        if not isinstance(rgb_std, (list, tuple)) or len(rgb_std) != 3:
            logger.warning(f"RGB std length is {len(rgb_std) if isinstance(rgb_std, (list, tuple)) else 'not a list'}, expected 3. Using ImageNet defaults.")
            rgb_std = [0.229, 0.224, 0.225]
        
        self.transform_eval = EvalTransformWrapper(
            self.transform_eval_rgb,
            input_size=rgb_input_size or [224, 384],
            mean=rgb_mean,
            std=rgb_std,
        )

    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_dataset = TennisUnlabelCombinedMMDataset(
                self.data_module_cfg, self.transform_train, self.mask_gen
            )
            
            # æ‰“å°è®­ç»ƒé›†ä¿¡æ¯
            train_size = len(self.train_dataset)
            print("=" * 80)
            print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
            print("=" * 80)
            print(f"âœ… è®­ç»ƒé›† (Train Dataset):")
            print(f"   - æ€»æ ·æœ¬æ•°: {train_size}")
            if hasattr(self.train_dataset, 'unlabel_loader'):
                if hasattr(self.train_dataset.unlabel_loader, '_dir_to_img_frame'):
                    print(f"   - è§†é¢‘/ç›®å½•æ•°: {len(self.train_dataset.unlabel_loader._dir_to_img_frame)}")
            
            self.batch_sampler_train = BatchSampler(
                sampler=DistributedSampler(
                    dataset=self.train_dataset,
                    num_replicas=dist.get_world_size() if dist.is_initialized() else 1,
                    rank=dist.get_rank() if dist.is_initialized() else 0,
                    shuffle=True,
                ),
                batch_size=self.cfg.batch_size,
                drop_last=True,
            )
            self.val_dataset = get_fewshot_eval_dataset(
                self.data_module_cfg.dataset,
                self.transform_eval,
                self.mask_gen,
                self.data_module_cfg.num_frames,
                "RGB",
            )
            
            # æ‰“å°éªŒè¯é›†ä¿¡æ¯
            val_size = len(self.val_dataset)
            print(f"\nâœ… éªŒè¯é›† (Validation Dataset):")
            print(f"   - æ€»æ ·æœ¬æ•°: {val_size}")
            
            # ç»Ÿè®¡æ¯ä¸ª action ç±»åˆ«çš„æ ·æœ¬æ•°
            if hasattr(self.val_dataset, '_action_idx') and hasattr(self.val_dataset, '_action_label'):
                action_counts = {}
                action_labels_map = {}
                
                # ç»Ÿè®¡æ¯ä¸ª action_idx çš„æ ·æœ¬æ•°
                for i, action_idx in enumerate(self.val_dataset._action_idx):
                    action_counts[action_idx] = action_counts.get(action_idx, 0) + 1
                    # è®°å½•æ¯ä¸ª action_idx å¯¹åº”çš„ label
                    if action_idx not in action_labels_map:
                        action_labels_map[action_idx] = self.val_dataset._action_label[i] if i < len(self.val_dataset._action_label) else "N/A"
                
                print(f"   - Action ç±»åˆ«æ•°: {len(action_counts)}")
                print(f"   - æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°:")
                for action_idx, count in sorted(action_counts.items()):
                    action_label = action_labels_map.get(action_idx, "N/A")
                    print(f"     * Action {action_idx} ({action_label}): {count} ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥ few-shot è¯„ä¼°çš„å¯è¡Œæ€§
            expected_batch_size = self.n_way * (self.k_shot + self.q_sample)
            print(f"\nğŸ“‹ Few-shot è¯„ä¼°é…ç½®:")
            print(f"   - N-way: {self.n_way}")
            print(f"   - K-shot: {self.k_shot}")
            print(f"   - Q-sample: {self.q_sample}")
            print(f"   - æœŸæœ›çš„ batch size: {expected_batch_size}")
            print(f"   - Episodes: {self.episodes}")
            
            if val_size < expected_batch_size:
                print(f"\nâš ï¸  è­¦å‘Š: éªŒè¯é›†æ ·æœ¬æ•° ({val_size}) å°äºæœŸæœ›çš„ batch size ({expected_batch_size})")
                print(f"   æ— æ³•åˆ›å»ºå®Œæ•´çš„ few-shot episodeã€‚")
                print(f"   å»ºè®®:")
                print(f"   1. å¢åŠ éªŒè¯æ•°æ®é›†çš„å¤§å°")
                print(f"   2. æˆ–è€…å‡å° n_way, k_shot, q_sample çš„å€¼")
            else:
                max_episodes = val_size // expected_batch_size
                print(f"   - ç†è®ºä¸Šå¯ä»¥åˆ›å»ºçš„æœ€å¤§ episodes: {max_episodes}")
                if self.episodes > max_episodes:
                    print(f"   âš ï¸  è­¦å‘Š: é…ç½®çš„ episodes ({self.episodes}) å¤§äºå¯åˆ›å»ºçš„æœ€å¤§å€¼ ({max_episodes})")
            
            print("=" * 80)
            batch_sampler = BatchSampler(
                sampler=DistributedSampler(
                    dataset=range(self.eval_batch_size * self.episodes),
                    num_replicas=dist.get_world_size() if dist.is_initialized() else 1,
                    rank=dist.get_rank() if dist.is_initialized() else 0,
                ),
                batch_size=self.eval_batch_size,
                drop_last=False,
            )
            self.episodic_batch_sampler_val = EpisodicBatchSampler(
                dataset=self.val_dataset,
                batch_sampler=batch_sampler,
                n_way=self.n_way,
                k_shot=self.k_shot,
                q_sample=self.q_sample,
                episodes=self.episodes,
            )
        elif stage == "test":
            self.test_dataset = get_fewshot_eval_dataset(
                self.data_module_cfg.dataset,
                self.transform_eval,
                self.mask_gen,
                self.data_module_cfg.num_frames,
                "RGB",
            )
            
            # ä¸º test stage åˆ›å»º batch_sampler
            batch_sampler = BatchSampler(
                sampler=DistributedSampler(
                    dataset=range(self.eval_batch_size * self.episodes),
                    num_replicas=dist.get_world_size() if dist.is_initialized() else 1,
                    rank=dist.get_rank() if dist.is_initialized() else 0,
                ),
                batch_size=self.eval_batch_size,
                drop_last=False,
            )
            self.episodic_batch_sampler_test = EpisodicBatchSampler(
                dataset=self.test_dataset,
                batch_sampler=batch_sampler,
                n_way=self.n_way,
                k_shot=self.k_shot,
                q_sample=self.q_sample,
                episodes=self.episodes,
            )

    def train_dataloader(self):
        # è¯„ä¼°é˜¶æ®µå¯èƒ½æ²¡æœ‰ train_dataset
        if not hasattr(self, 'train_dataset') or self.train_dataset is None:
            return None
        return DataLoader(
            self.train_dataset,
            batch_sampler=self.batch_sampler_train,
            num_workers=self.cfg.num_workers,
        )

    def val_dataloader(self):
        # æµ‹è¯•é˜¶æ®µå¯èƒ½æ²¡æœ‰ val_dataset
        if not hasattr(self, 'val_dataset') or self.val_dataset is None:
            return None
        return DataLoader(
            self.val_dataset,
            batch_sampler=self.episodic_batch_sampler_val,
            num_workers=self.cfg.num_workers,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_sampler=self.episodic_batch_sampler_test,
            num_workers=self.cfg.num_workers,
        )
