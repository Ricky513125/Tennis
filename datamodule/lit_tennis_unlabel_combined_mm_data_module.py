"""
Tennis æ•°æ®é›†çš„å¤šæ¨¡æ€æ•°æ®æ¨¡å—ï¼ˆRGBã€Flowã€Skeletonï¼‰
ç”¨äºå¤šæ¨¡æ€è’¸é¦è®­ç»ƒ
"""
import pytorch_lightning as pl
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data.sampler import BatchSampler

from datamodule.dataset.tennis_unlabel_combined_multimodal_dataset import (
    TennisUnlabelCombinedMMDataset,
)
from datamodule.utils.augmentation import (
    DataAugmentationForUnlabelMM,
    DataAugmentationForUnlabelRGB,
    DataAugmentationForVideoMAERGB,
    MaskGeneration,
)
from datamodule.utils.episodic_batch_sampler import EpisodicBatchSampler
from netscripts.get_fewshot_eval_dataset import get_fewshot_eval_dataset


class TennisUnlabelCombinedMMDataModule(pl.LightningDataModule):
    def __init__(self, cfg):
        super(TennisUnlabelCombinedMMDataModule, self).__init__()
        self.cfg = cfg
        self.data_module_cfg = cfg.data_module
        self.n_way = cfg.data_module.n_way
        self.k_shot = cfg.data_module.k_shot
        self.q_sample = cfg.data_module.q_sample
        self.episodes = cfg.data_module.episodes
        self.eval_batch_size = self.n_way * (self.k_shot + self.q_sample)
        
        # transform
        self.mask_gen = MaskGeneration(cfg.data_module)
        self.transform_train_rgb = DataAugmentationForUnlabelRGB(cfg.data_module)
        self.transform_train_flow = DataAugmentationForUnlabelMM(
            cfg.data_module, mean=cfg.data_module.mean[1], std=cfg.data_module.std[1], mode="flow"
        )
        self.transform_train_skeleton = DataAugmentationForUnlabelMM(
            cfg.data_module, mean=cfg.data_module.mean[2], std=cfg.data_module.std[2], mode="skeleton"
        )
        
        self.transform_train = [
            self.transform_train_rgb,
            self.transform_train_flow,
            self.transform_train_skeleton,
        ]
        
        # è¯„ä¼°ç”¨çš„ transformï¼ˆåªä½¿ç”¨ RGBï¼‰
        # éœ€è¦ä»é…ç½®ä¸­è·å– input_sizeï¼ŒTennis ä½¿ç”¨ [224, 384]
        input_size = cfg.data_module.input_size[0] if isinstance(cfg.data_module.input_size, list) else 224
        if isinstance(input_size, list):
            input_size = input_size[0]  # å–é«˜åº¦
        
        self.transform_eval_rgb = DataAugmentationForVideoMAERGB(
            cfg.data_module, 
            input_size=input_size,
            multi_scale_crop=False
        )
        self.transform_eval = self.transform_eval_rgb

    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_dataset = TennisUnlabelCombinedMMDataset(
                self.data_module_cfg, self.transform_train, self.mask_gen
            )
            
            # æ‰“å°è®­ç»ƒé›†ä¿¡æ¯
            train_size = len(self.train_dataset)
            print("=" * 80)
            print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
            print("=" * 80)
            print(f"âœ… è®­ç»ƒé›† (Train Dataset):")
            print(f"   - æ€»æ ·æœ¬æ•°: {train_size}")
            if hasattr(self.train_dataset, 'unlabel_loader'):
                if hasattr(self.train_dataset.unlabel_loader, '_dir_to_img_frame'):
                    print(f"   - è§†é¢‘/ç›®å½•æ•°: {len(self.train_dataset.unlabel_loader._dir_to_img_frame)}")
            
            self.batch_sampler_train = BatchSampler(
                sampler=DistributedSampler(
                    dataset=self.train_dataset,
                    num_replicas=dist.get_world_size() if dist.is_initialized() else 1,
                    rank=dist.get_rank() if dist.is_initialized() else 0,
                    shuffle=True,
                ),
                batch_size=self.cfg.batch_size,
                drop_last=True,
            )
            self.val_dataset = get_fewshot_eval_dataset(
                self.data_module_cfg.dataset,
                self.transform_eval,
                self.mask_gen,
                self.data_module_cfg.num_frames,
                "RGB",
            )
            
            # æ‰“å°éªŒè¯é›†ä¿¡æ¯
            val_size = len(self.val_dataset)
            print(f"\nâœ… éªŒè¯é›† (Validation Dataset):")
            print(f"   - æ€»æ ·æœ¬æ•°: {val_size}")
            
            # ç»Ÿè®¡æ¯ä¸ª action ç±»åˆ«çš„æ ·æœ¬æ•°
            if hasattr(self.val_dataset, '_action_idx') and hasattr(self.val_dataset, '_action_label'):
                action_counts = {}
                action_labels_map = {}
                
                # ç»Ÿè®¡æ¯ä¸ª action_idx çš„æ ·æœ¬æ•°
                for i, action_idx in enumerate(self.val_dataset._action_idx):
                    action_counts[action_idx] = action_counts.get(action_idx, 0) + 1
                    # è®°å½•æ¯ä¸ª action_idx å¯¹åº”çš„ label
                    if action_idx not in action_labels_map:
                        action_labels_map[action_idx] = self.val_dataset._action_label[i] if i < len(self.val_dataset._action_label) else "N/A"
                
                print(f"   - Action ç±»åˆ«æ•°: {len(action_counts)}")
                print(f"   - æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°:")
                for action_idx, count in sorted(action_counts.items()):
                    action_label = action_labels_map.get(action_idx, "N/A")
                    print(f"     * Action {action_idx} ({action_label}): {count} ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥ few-shot è¯„ä¼°çš„å¯è¡Œæ€§
            expected_batch_size = self.n_way * (self.k_shot + self.q_sample)
            print(f"\nğŸ“‹ Few-shot è¯„ä¼°é…ç½®:")
            print(f"   - N-way: {self.n_way}")
            print(f"   - K-shot: {self.k_shot}")
            print(f"   - Q-sample: {self.q_sample}")
            print(f"   - æœŸæœ›çš„ batch size: {expected_batch_size}")
            print(f"   - Episodes: {self.episodes}")
            
            if val_size < expected_batch_size:
                print(f"\nâš ï¸  è­¦å‘Š: éªŒè¯é›†æ ·æœ¬æ•° ({val_size}) å°äºæœŸæœ›çš„ batch size ({expected_batch_size})")
                print(f"   æ— æ³•åˆ›å»ºå®Œæ•´çš„ few-shot episodeã€‚")
                print(f"   å»ºè®®:")
                print(f"   1. å¢åŠ éªŒè¯æ•°æ®é›†çš„å¤§å°")
                print(f"   2. æˆ–è€…å‡å° n_way, k_shot, q_sample çš„å€¼")
            else:
                max_episodes = val_size // expected_batch_size
                print(f"   - ç†è®ºä¸Šå¯ä»¥åˆ›å»ºçš„æœ€å¤§ episodes: {max_episodes}")
                if self.episodes > max_episodes:
                    print(f"   âš ï¸  è­¦å‘Š: é…ç½®çš„ episodes ({self.episodes}) å¤§äºå¯åˆ›å»ºçš„æœ€å¤§å€¼ ({max_episodes})")
            
            print("=" * 80)
            batch_sampler = BatchSampler(
                sampler=DistributedSampler(
                    dataset=range(self.eval_batch_size * self.episodes),
                    num_replicas=dist.get_world_size() if dist.is_initialized() else 1,
                    rank=dist.get_rank() if dist.is_initialized() else 0,
                ),
                batch_size=self.eval_batch_size,
                drop_last=False,
            )
            self.episodic_batch_sampler_val = EpisodicBatchSampler(
                dataset=self.val_dataset,
                batch_sampler=batch_sampler,
                n_way=self.n_way,
                k_shot=self.k_shot,
                q_sample=self.q_sample,
                episodes=self.episodes,
            )
        elif stage == "test":
            self.test_dataset = get_fewshot_eval_dataset(
                self.data_module_cfg.dataset,
                self.transform_eval,
                self.mask_gen,
                self.data_module_cfg.num_frames,
                "RGB",
            )
            self.episodic_batch_sampler_test = EpisodicBatchSampler(
                dataset=self.test_dataset,
                n_way=self.n_way,
                k_shot=self.k_shot,
                q_sample=self.q_sample,
                episodes=self.episodes,
            )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_sampler=self.batch_sampler_train,
            num_workers=self.cfg.num_workers,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_sampler=self.episodic_batch_sampler_val,
            num_workers=self.cfg.num_workers,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_sampler=self.episodic_batch_sampler_test,
            num_workers=self.cfg.num_workers,
        )
